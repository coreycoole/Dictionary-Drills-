{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Challenge\n",
    "- The Fashion MNIST Dataset is taken in and processed for modeling through TensorFlow and the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "#from pyimagesearch.minivggnet import MiniVGGNet\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGGNet:\n",
    "\n",
    "    def build_a(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    " \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        # first CONV => RELU => CONV => RELU => POOL layer set\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # second CONV => RELU => CONV => RELU => POOL layer set\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model\n",
    "\n",
    "    def build_b(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    " \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        \n",
    "        # first CONV => RELU => CONV => RELU => POOL layer set\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model\n",
    "    \n",
    "    def build_1(width, height, depth, classes): \n",
    "        \n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        \n",
    "        # Must define the input shape in the first layer of the neural network\n",
    "        model.add(Dense(64, activation='relu', input_shape=inputShape))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model\n",
    "    \n",
    "    def build_2(width, height, depth, classes): \n",
    "        \n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        \n",
    "        # Must define the input shape in the first layer of the neural network\n",
    "        model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=inputShape)) \n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model\n",
    "    \n",
    "    def build_3(width, height, depth, classes): \n",
    "        \n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        \n",
    "        # Must define the input shape in the first layer of the neural network\n",
    "        model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=inputShape)) \n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu')) \n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Fashion Dataset, split testing and training groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Fashion MNIST...\n"
     ]
    }
   ],
   "source": [
    "# grab the Fashion MNIST dataset (if this is your first time running\n",
    "# this the dataset will be automatically downloaded)\n",
    "print(\"[INFO] loading Fashion MNIST...\")\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    " \n",
    "# if we are using \"channels first\" ordering, then reshape the design\n",
    "# matrix such that the matrix is:\n",
    "# num_samples x depth x rows x columns\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    trainX = trainX.reshape((trainX.shape[0], 1, 28, 28))\n",
    "    testX = testX.reshape((testX.shape[0], 1, 28, 28))\n",
    " \n",
    "# otherwise, we are using \"channels last\" ordering, so the design\n",
    "# matrix shape should be: num_samples x rows x columns x depth\n",
    "else:\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to the range of [0, 1]\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    " \n",
    "# one-hot encode the training and testing labels\n",
    "trainY = np_utils.to_categorical(trainY, 10)\n",
    "testY = np_utils.to_categorical(testY, 10)\n",
    " \n",
    "# initialize the label names\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "    \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Baseline Dense MLP with categorical_crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 86s 1ms/step - loss: 0.6996 - acc: 0.7440 - val_loss: 0.4803 - val_acc: 0.8256\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 92s 2ms/step - loss: 0.5581 - acc: 0.7975 - val_loss: 0.4484 - val_acc: 0.8354\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 0.5248 - acc: 0.8091 - val_loss: 0.4345 - val_acc: 0.8438\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 0.5057 - acc: 0.8160 - val_loss: 0.4347 - val_acc: 0.8384\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.4969 - acc: 0.8206 - val_loss: 0.4177 - val_acc: 0.8492\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.4860 - acc: 0.8238 - val_loss: 0.4139 - val_acc: 0.8517\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.4805 - acc: 0.8259 - val_loss: 0.4135 - val_acc: 0.8513\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.4748 - acc: 0.8296 - val_loss: 0.4091 - val_acc: 0.8525\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.4684 - acc: 0.8314 - val_loss: 0.4018 - val_acc: 0.8530\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.4639 - acc: 0.8319 - val_loss: 0.4031 - val_acc: 0.8547\n",
      "Test loss: 0.40305262362957\n",
      "Test accuracy: 0.8547\n",
      "Test loss: 0.40305262362957\n",
      "Test accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = MiniVGGNet.build_1(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "H = model.fit(trainX, trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    batch_size=BS, epochs=NUM_EPOCHS)\n",
    "\n",
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Change loss function to binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 152s 3ms/step - loss: 0.1335 - acc: 0.9476 - val_loss: 0.0902 - val_acc: 0.9635\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.1007 - acc: 0.9602 - val_loss: 0.0834 - val_acc: 0.9666\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0944 - acc: 0.9631 - val_loss: 0.0819 - val_acc: 0.9671\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.0918 - acc: 0.9640 - val_loss: 0.0796 - val_acc: 0.9678\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0899 - acc: 0.9649 - val_loss: 0.0782 - val_acc: 0.9686\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0886 - acc: 0.9653 - val_loss: 0.0777 - val_acc: 0.9689\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0875 - acc: 0.9659 - val_loss: 0.0769 - val_acc: 0.9691\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.0871 - acc: 0.9659 - val_loss: 0.0766 - val_acc: 0.9693\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0857 - acc: 0.9666 - val_loss: 0.0759 - val_acc: 0.9694\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.0860 - acc: 0.9665 - val_loss: 0.0755 - val_acc: 0.9700\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-235e0b8244e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     batch_size=BS, epochs=NUM_EPOCHS)\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = MiniVGGNet.build_1(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "H = model.fit(trainX, trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    batch_size=BS, epochs=NUM_EPOCHS)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07549896616339684\n",
      "Test accuracy: 0.9699600028038025\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Adding a Convolution layer with the categorical_crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.7253 - acc: 0.7262 - val_loss: 0.4929 - val_acc: 0.8203\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 140s 2ms/step - loss: 0.5114 - acc: 0.8112 - val_loss: 0.4171 - val_acc: 0.8484\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.4653 - acc: 0.8275 - val_loss: 0.3924 - val_acc: 0.8575\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.4424 - acc: 0.8391 - val_loss: 0.3780 - val_acc: 0.8603\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 93s 2ms/step - loss: 0.4309 - acc: 0.8423 - val_loss: 0.3666 - val_acc: 0.8666\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.4221 - acc: 0.8467 - val_loss: 0.3597 - val_acc: 0.8684\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.4107 - acc: 0.8506 - val_loss: 0.3556 - val_acc: 0.8697\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 0.4113 - acc: 0.8511 - val_loss: 0.3504 - val_acc: 0.8721\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.4017 - acc: 0.8536 - val_loss: 0.3477 - val_acc: 0.8738\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 96s 2ms/step - loss: 0.3987 - acc: 0.8528 - val_loss: 0.3425 - val_acc: 0.8750\n",
      "Test loss: 0.34248134772777555\n",
      "Test accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = MiniVGGNet.build_2(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "H = model.fit(trainX, trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    batch_size=BS, epochs=NUM_EPOCHS)\n",
    "\n",
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Adding Convolution layer with binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 96s 2ms/step - loss: 0.1339 - acc: 0.9468 - val_loss: 0.0932 - val_acc: 0.9626\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 116s 2ms/step - loss: 0.0962 - acc: 0.9617 - val_loss: 0.0851 - val_acc: 0.9658\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.0899 - acc: 0.9643 - val_loss: 0.0812 - val_acc: 0.9674\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 150s 3ms/step - loss: 0.0864 - acc: 0.9657 - val_loss: 0.0793 - val_acc: 0.9680\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 144s 2ms/step - loss: 0.0843 - acc: 0.9664 - val_loss: 0.0774 - val_acc: 0.9687\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 0.0826 - acc: 0.9673 - val_loss: 0.0764 - val_acc: 0.9694\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0813 - acc: 0.9681 - val_loss: 0.0753 - val_acc: 0.9696\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0805 - acc: 0.9680 - val_loss: 0.0745 - val_acc: 0.9702\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.0795 - acc: 0.9684 - val_loss: 0.0740 - val_acc: 0.9704\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0785 - acc: 0.9691 - val_loss: 0.0738 - val_acc: 0.9701\n",
      "Test loss: 0.07380805897712707\n",
      "Test accuracy: 0.9700500040054322\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = MiniVGGNet.build_2(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "H = model.fit(trainX, trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    batch_size=BS, epochs=NUM_EPOCHS)\n",
    "\n",
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Adding Second Convolution layer with binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 108s 2ms/step - loss: 0.1703 - acc: 0.9304 - val_loss: 0.1213 - val_acc: 0.9480\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.1272 - acc: 0.9468 - val_loss: 0.1107 - val_acc: 0.9535\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 115s 2ms/step - loss: 0.1191 - acc: 0.9498 - val_loss: 0.1072 - val_acc: 0.9547\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.1145 - acc: 0.9523 - val_loss: 0.1032 - val_acc: 0.9567\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 99s 2ms/step - loss: 0.1113 - acc: 0.9533 - val_loss: 0.1007 - val_acc: 0.9582\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.1095 - acc: 0.9540 - val_loss: 0.0988 - val_acc: 0.9591\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.1074 - acc: 0.9553 - val_loss: 0.0984 - val_acc: 0.9590\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.1062 - acc: 0.9562 - val_loss: 0.0968 - val_acc: 0.9599\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 137s 2ms/step - loss: 0.1047 - acc: 0.9566 - val_loss: 0.0949 - val_acc: 0.9610\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.1042 - acc: 0.9569 - val_loss: 0.0948 - val_acc: 0.9606\n",
      "Test loss: 0.0947646532356739\n",
      "Test accuracy: 0.9606400032997131\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 10\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS, momentum=0.9, nesterov=True)\n",
    "\n",
    "model = MiniVGGNet.build_3(width=28, height=28, depth=1, classes=10)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    " \n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "H = model.fit(trainX, trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    batch_size=BS, epochs=NUM_EPOCHS)\n",
    "\n",
    "score = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model.save('f_mnist_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our baseline accuracy spread is .8319/.8547, indicating an amount of underfit. Throughout the model development, we see that higher accuracy is obtained with the binary_crossentropy loss function. \n",
    "\n",
    "- We reached our highest accuracy levels and highest generalization at .9691/.97005, when a Convolution layer was added as the top layer in the network. However adding a second convolution layer behind the first did not help our model's accuracy. We may look into batchnormalization to remedy this result. \n",
    "\n",
    "- Although model 4 was the most accurate, it was also the most computationally expensive at a total of 1294 seonds to process. However further parameters would have to be tested to define the curve of the time complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "preds = model.predict(testX)\n",
    " \n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1),\n",
    "    target_names=labelNames))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot_1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
